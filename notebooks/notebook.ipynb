{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2d3c81",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# üé≠ Enhanced DistilBERT Sentiment Analysis - Demo Notebook\\n\",\n",
    "    \"\\n\",\n",
    "    \"This notebook demonstrates how to use the trained Enhanced DistilBERT model for sentiment analysis.\\n\",\n",
    "    \"\\n\",\n",
    "    \"**Model Performance:**\\n\",\n",
    "    \"- Accuracy: 95.17%\\n\",\n",
    "    \"- Dataset: SST-2 (67,349 samples)\\n\",\n",
    "    \"- Architecture: DistilBERT + LoRA Adapters\\n\",\n",
    "    \"- Training: Knowledge Distillation from BERT-base\\n\",\n",
    "    \"\\n\",\n",
    "    \"**Author:** Your Name  \\n\",\n",
    "    \"**Date:** 2024  \\n\",\n",
    "    \"**GitHub:** [Your Repository](https://github.com/yourusername/sentiment-analysis-distilbert)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## üì¶ 1. Setup and Installation\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Install required packages (run only once)\\n\",\n",
    "    \"!pip install torch transformers pandas matplotlib seaborn plotly -q\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import torch\\n\",\n",
    "    \"import torch.nn as nn\\n\",\n",
    "    \"import torch.nn.functional as F\\n\",\n",
    "    \"from transformers import DistilBertModel, DistilBertTokenizer\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"import seaborn as sns\\n\",\n",
    "    \"from typing import Dict, List\\n\",\n",
    "    \"import warnings\\n\",\n",
    "    \"\\n\",\n",
    "    \"warnings.filterwarnings('ignore')\\n\",\n",
    "    \"sns.set_style('whitegrid')\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"‚úÖ All packages imported successfully!\\\")\\n\",\n",
    "    \"print(f\\\"PyTorch version: {torch.__version__}\\\")\\n\",\n",
    "    \"print(f\\\"CUDA available: {torch.cuda.is_available()}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## üèóÔ∏è 2. Model Architecture\\n\",\n",
    "    \"\\n\",\n",
    "    \"The model uses:\\n\",\n",
    "    \"- **LoRA (Low-Rank Adaptation)** for parameter-efficient fine-tuning\\n\",\n",
    "    \"- **Task-specific adapters** for enhanced performance\\n\",\n",
    "    \"- **Enhanced classifier head** with multiple layers\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"class LoRALayer(nn.Module):\\n\",\n",
    "    \"    \\\"\\\"\\\"Low-Rank Adaptation layer\\\"\\\"\\\"\\n\",\n",
    "    \"    def __init__(self, in_features: int, out_features: int, rank: int = 8, alpha: float = 16):\\n\",\n",
    "    \"        super().__init__()\\n\",\n",
    "    \"        self.rank = rank\\n\",\n",
    "    \"        self.scaling = alpha / rank\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        self.lora_A = nn.Parameter(torch.zeros(in_features, rank))\\n\",\n",
    "    \"        self.lora_B = nn.Parameter(torch.zeros(rank, out_features))\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        nn.init.kaiming_uniform_(self.lora_A, a=np.sqrt(5))\\n\",\n",
    "    \"        nn.init.zeros_(self.lora_B)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    def forward(self, x):\\n\",\n",
    "    \"        return (x @ self.lora_A @ self.lora_B) * self.scaling\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"class EnhancedDistilBERT(nn.Module):\\n\",\n",
    "    \"    \\\"\\\"\\\"Enhanced DistilBERT with LoRA adapters\\\"\\\"\\\"\\n\",\n",
    "    \"    def __init__(self, model_name: str = \\\"distilbert-base-uncased\\\", num_labels: int = 2,\\n\",\n",
    "    \"                 adapter_size: int = 64, lora_rank: int = 8, dropout: float = 0.1):\\n\",\n",
    "    \"        super().__init__()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        self.distilbert = DistilBertModel.from_pretrained(model_name)\\n\",\n",
    "    \"        config = self.distilbert.config\\n\",\n",
    "    \"        hidden_size = config.hidden_size\\n\",\n",
    "    \"        num_layers = config.n_layers\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Freeze base model\\n\",\n",
    "    \"        for param in self.distilbert.parameters():\\n\",\n",
    "    \"            param.requires_grad = False\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        for name, param in self.distilbert.named_parameters():\\n\",\n",
    "    \"            if 'LayerNorm' in name:\\n\",\n",
    "    \"                param.requires_grad = True\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # LoRA layers\\n\",\n",
    "    \"        self.lora_layers = nn.ModuleList([\\n\",\n",
    "    \"            LoRALayer(hidden_size, hidden_size, rank=lora_rank)\\n\",\n",
    "    \"            for _ in range(num_layers)\\n\",\n",
    "    \"        ])\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Adapters\\n\",\n",
    "    \"        self.adapters = nn.ModuleList([\\n\",\n",
    "    \"            nn.Sequential(\\n\",\n",
    "    \"                nn.Linear(hidden_size, adapter_size),\\n\",\n",
    "    \"                nn.GELU(),\\n\",\n",
    "    \"                nn.Dropout(dropout),\\n\",\n",
    "    \"                nn.Linear(adapter_size, hidden_size),\\n\",\n",
    "    \"                nn.LayerNorm(hidden_size)\\n\",\n",
    "    \"            ) for _ in range(num_layers)\\n\",\n",
    "    \"        ])\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Classifier\\n\",\n",
    "    \"        self.classifier = nn.Sequential(\\n\",\n",
    "    \"            nn.Linear(hidden_size, hidden_size),\\n\",\n",
    "    \"            nn.LayerNorm(hidden_size),\\n\",\n",
    "    \"            nn.GELU(),\\n\",\n",
    "    \"            nn.Dropout(dropout * 2),\\n\",\n",
    "    \"            nn.Linear(hidden_size, hidden_size // 2),\\n\",\n",
    "    \"            nn.GELU(),\\n\",\n",
    "    \"            nn.Dropout(dropout),\\n\",\n",
    "    \"            nn.Linear(hidden_size // 2, num_labels)\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        for module in self.classifier:\\n\",\n",
    "    \"            if isinstance(module, nn.Linear):\\n\",\n",
    "    \"                nn.init.xavier_uniform_(module.weight)\\n\",\n",
    "    \"                nn.init.zeros_(module.bias)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    def forward(self, input_ids, attention_mask):\\n\",\n",
    "    \"        outputs = self.distilbert(\\n\",\n",
    "    \"            input_ids=input_ids,\\n\",\n",
    "    \"            attention_mask=attention_mask,\\n\",\n",
    "    \"            output_hidden_states=True\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        hidden_states = outputs.hidden_states\\n\",\n",
    "    \"        x = hidden_states[-1]\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        for i, (lora, adapter) in enumerate(zip(self.lora_layers, self.adapters)):\\n\",\n",
    "    \"            layer_hidden = hidden_states[min(i + 1, len(hidden_states) - 1)]\\n\",\n",
    "    \"            x_lora = lora(layer_hidden)\\n\",\n",
    "    \"            x_adapter = adapter(layer_hidden)\\n\",\n",
    "    \"            x = layer_hidden + x_lora + x_adapter\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        pooled = x[:, 0]\\n\",\n",
    "    \"        logits = self.classifier(pooled)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        return logits\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"‚úÖ Model architecture defined!\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## üì• 3. Load Trained Model\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Configuration\\n\",\n",
    "    \"MODEL_PATH = 'models/best_model_sst2.pt'\\n\",\n",
    "    \"DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Loading model from: {MODEL_PATH}\\\")\\n\",\n",
    "    \"print(f\\\"Using device: {DEVICE}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Load checkpoint\\n\",\n",
    "    \"checkpoint = torch.load(MODEL_PATH, map_location=DEVICE, weights_only=False)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Initialize model\\n\",\n",
    "    \"model = EnhancedDistilBERT(num_labels=2)\\n\",\n",
    "    \"model.load_state_dict(checkpoint['model_state_dict'])\\n\",\n",
    "    \"model.to(DEVICE)\\n\",\n",
    "    \"model.eval()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Load tokenizer\\n\",\n",
    "    \"tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Model metadata\\n\",\n",
    "    \"accuracy = checkpoint.get('accuracy', 0.0)\\n\",\n",
    "    \"epoch = checkpoint.get('epoch', 0)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\n\\\" + \\\"=\\\"*50)\\n\",\n",
    "    \"print(\\\"‚úÖ MODEL LOADED SUCCESSFULLY!\\\")\\n\",\n",
    "    \"print(\\\"=\\\"*50)\\n\",\n",
    "    \"print(f\\\"üìä Validation Accuracy: {accuracy*100:.2f}%\\\")\\n\",\n",
    "    \"print(f\\\"üìö Training Epoch: {epoch}\\\")\\n\",\n",
    "    \"print(f\\\"üîß Device: {DEVICE}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Count parameters\\n\",\n",
    "    \"total_params = sum(p.numel() for p in model.parameters())\\n\",\n",
    "    \"trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\\n\",\n",
    "    \"print(f\\\"\\\\nüìà Total Parameters: {total_params:,}\\\")\\n\",\n",
    "    \"print(f\\\"üéØ Trainable Parameters: {trainable_params:,}\\\")\\n\",\n",
    "    \"print(f\\\"‚ö° Parameter Efficiency: {100*trainable_params/total_params:.1f}%\\\")\\n\",\n",
    "    \"print(\\\"=\\\"*50)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## üîÆ 4. Prediction Function\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"def predict_sentiment(text: str, verbose: bool = True) -> Dict:\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    Predict sentiment for a given text\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    Args:\\n\",\n",
    "    \"        text: Input text to analyze\\n\",\n",
    "    \"        verbose: Whether to print detailed output\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    Returns:\\n\",\n",
    "    \"        Dictionary containing prediction results\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    # Tokenize\\n\",\n",
    "    \"    inputs = tokenizer(\\n\",\n",
    "    \"        text,\\n\",\n",
    "    \"        return_tensors='pt',\\n\",\n",
    "    \"        padding=True,\\n\",\n",
    "    \"        truncation=True,\\n\",\n",
    "    \"        max_length=128\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Predict\\n\",\n",
    "    \"    with torch.no_grad():\\n\",\n",
    "    \"        logits = model(inputs['input_ids'], inputs['attention_mask'])\\n\",\n",
    "    \"        probs = F.softmax(logits, dim=-1)\\n\",\n",
    "    \"        pred = probs.argmax(dim=-1)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    result = {\\n\",\n",
    "    \"        'text': text,\\n\",\n",
    "    \"        'sentiment': 'positive' if pred.item() == 1 else 'negative',\\n\",\n",
    "    \"        'label': int(pred.item()),\\n\",\n",
    "    \"        'confidence': float(probs[0][pred.item()]),\\n\",\n",
    "    \"        'probabilities': {\\n\",\n",
    "    \"            'negative': float(probs[0][0]),\\n\",\n",
    "    \"            'positive': float(probs[0][1])\\n\",\n",
    "    \"        }\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    if verbose:\\n\",\n",
    "    \"        print(\\\"\\\\n\\\" + \\\"=\\\"*60)\\n\",\n",
    "    \"        print(\\\"üìù TEXT:\\\", text)\\n\",\n",
    "    \"        print(\\\"-\\\" * 60)\\n\",\n",
    "    \"        emoji = \\\"üòä\\\" if result['sentiment'] == 'positive' else \\\"üòû\\\"\\n\",\n",
    "    \"        print(f\\\"{emoji} SENTIMENT: {result['sentiment'].upper()}\\\")\\n\",\n",
    "    \"        print(f\\\"üìä Confidence: {result['confidence']*100:.2f}%\\\")\\n\",\n",
    "    \"        print(f\\\"üìà Probabilities:\\\")\\n\",\n",
    "    \"        print(f\\\"   ‚Ä¢ Negative: {result['probabilities']['negative']*100:.2f}%\\\")\\n\",\n",
    "    \"        print(f\\\"   ‚Ä¢ Positive: {result['probabilities']['positive']*100:.2f}%\\\")\\n\",\n",
    "    \"        print(\\\"=\\\"*60)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return result\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"def predict_batch(texts: List[str]) -> pd.DataFrame:\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    Predict sentiment for multiple texts\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    Args:\\n\",\n",
    "    \"        texts: List of texts to analyze\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    Returns:\\n\",\n",
    "    \"        DataFrame with results\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    results = []\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    for text in texts:\\n\",\n",
    "    \"        result = predict_sentiment(text, verbose=False)\\n\",\n",
    "    \"        results.append(result)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    df = pd.DataFrame(results)\\n\",\n",
    "    \"    return df\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"‚úÖ Prediction functions ready!\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## üéØ 5. Example Predictions\\n\",\n",
    "    \"\\n\",\n",
    "    \"Let's test the model on various examples!\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Example 1: Positive sentiment\\n\",\n",
    "    \"result1 = predict_sentiment(\\\"This movie was absolutely fantastic! I loved every moment of it.\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Example 2: Negative sentiment\\n\",\n",
    "    \"result2 = predict_sentiment(\\\"Terrible film. Complete waste of time and money.\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Example 3: Neutral/Mixed sentiment\\n\",\n",
    "    \"result3 = predict_sentiment(\\\"The acting was good but the plot was confusing.\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Example 4: Your own text!\\n\",\n",
    "    \"custom_text = \\\"I'm so happy with this purchase! Highly recommend it.\\\"\\n\",\n",
    "    \"result4 = predict_sentiment(custom_text)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## üìä 6. Batch Analysis with Visualization\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Sample movie reviews\\n\",\n",
    "    \"sample_texts = [\\n\",\n",
    "    \"    \\\"This movie was absolutely fantastic!\\\",\\n\",\n",
    "    \"    \\\"Worst film I've ever seen.\\\",\\n\",\n",
    "    \"    \\\"Pretty good, would watch again.\\\",\\n\",\n",
    "    \"    \\\"Boring and predictable.\\\",\\n\",\n",
    "    \"    \\\"Amazing performances by the cast!\\\",\\n\",\n",
    "    \"    \\\"Not worth the ticket price.\\\",\\n\",\n",
    "    \"    \\\"One of the best films this year!\\\",\\n\",\n",
    "    \"    \\\"Disappointing ending.\\\",\\n\",\n",
    "    \"    \\\"Exceeded my expectations!\\\",\\n\",\n",
    "    \"    \\\"Could have been better.\\\"\\n\",\n",
    "    \"]\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Get predictions\\n\",\n",
    "    \"results_df = predict_batch(sample_texts)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Display results\\n\",\n",
    "    \"print(\\\"\\\\nüìã BATCH PREDICTION RESULTS:\\\")\\n\",\n",
    "    \"print(\\\"=\\\"*80)\\n\",\n",
    "    \"display(results_df[['text', 'sentiment', 'confidence']].style.background_gradient(\\n\",\n",
    "    \"    subset=['confidence'], cmap='RdYlGn'\\n\",\n",
    "    \"))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Visualize sentiment distribution\\n\",\n",
    "    \"fig, axes = plt.subplots(1, 2, figsize=(14, 5))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Pie chart\\n\",\n",
    "    \"sentiment_counts = results_df['sentiment'].value_counts()\\n\",\n",
    "    \"colors = ['#38ef7d', '#eb3349']\\n\",\n",
    "    \"axes[0].pie(sentiment_counts.values, labels=sentiment_counts.index, autopct='%1.1f%%',\\n\",\n",
    "    \"            colors=colors, startangle=90, textprops={'fontsize': 12, 'weight': 'bold'})\\n\",\n",
    "    \"axes[0].set_title('Sentiment Distribution', fontsize=14, weight='bold')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Confidence distribution\\n\",\n",
    "    \"axes[1].hist(results_df['confidence'], bins=10, color='#667eea', edgecolor='white', alpha=0.8)\\n\",\n",
    "    \"axes[1].set_xlabel('Confidence', fontsize=12)\\n\",\n",
    "    \"axes[1].set_ylabel('Frequency', fontsize=12)\\n\",\n",
    "    \"axes[1].set_title('Confidence Distribution', fontsize=14, weight='bold')\\n\",\n",
    "    \"axes[1].grid(axis='y', alpha=0.3)\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.tight_layout()\\n\",\n",
    "    \"plt.show()\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"\\\\nüìä Summary Statistics:\\\")\\n\",\n",
    "    \"print(f\\\"   Positive: {sentiment_counts.get('positive', 0)} ({sentiment_counts.get('positive', 0)/len(results_df)*100:.1f}%)\\\")\\n\",\n",
    "    \"print(f\\\"   Negative: {sentiment_counts.get('negative', 0)} ({sentiment_counts.get('negative', 0)/len(results_df)*100:.1f}%)\\\")\\n\",\n",
    "    \"print(f\\\"   Average Confidence: {results_df['confidence'].mean()*100:.2f}%\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## üé® 7. Interactive Probability Visualization\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import plotly.graph_objects as go\\n\",\n",
    "    \"from plotly.subplots import make_subplots\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create subplots\\n\",\n",
    "    \"fig = make_subplots(\\n\",\n",
    "    \"    rows=1, cols=2,\\n\",\n",
    "    \"    subplot_titles=('Sentiment Probabilities', 'Confidence Scores'),\\n\",\n",
    "    \"    specs=[[{\\\"type\\\": \\\"bar\\\"}, {\\\"type\\\": \\\"scatter\\\"}]]\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Add probability bars\\n\",\n",
    "    \"for i, row in results_df.iterrows():\\n\",\n",
    "    \"    fig.add_trace(\\n\",\n",
    "    \"        go.Bar(\\n\",\n",
    "    \"            name=f\\\"Text {i+1}\\\",\\n\",\n",
    "    \"            x=['Negative', 'Positive'],\\n\",\n",
    "    \"            y=[row['probabilities']['negative'], row['probabilities']['positive']],\\n\",\n",
    "    \"            showlegend=False\\n\",\n",
    "    \"        ),\\n\",\n",
    "    \"        row=1, col=1\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Add confidence scatter\\n\",\n",
    "    \"colors = ['#38ef7d' if s == 'positive' else '#eb3349' for s in results_df['sentiment']]\\n\",\n",
    "    \"fig.add_trace(\\n\",\n",
    "    \"    go.Scatter(\\n\",\n",
    "    \"        x=list(range(1, len(results_df)+1)),\\n\",\n",
    "    \"        y=results_df['confidence'],\\n\",\n",
    "    \"        mode='markers+lines',\\n\",\n",
    "    \"        marker=dict(size=12, color=colors, line=dict(width=2, color='white')),\\n\",\n",
    "    \"        line=dict(color='gray', width=1, dash='dot'),\\n\",\n",
    "    \"        showlegend=False\\n\",\n",
    "    \"    ),\\n\",\n",
    "    \"    row=1, col=2\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"fig.update_xaxes(title_text=\\\"Sentiment\\\", row=1, col=1)\\n\",\n",
    "    \"fig.update_xaxes(title_text=\\\"Text Number\\\", row=1, col=2)\\n\",\n",
    "    \"fig.update_yaxes(title_text=\\\"Probability\\\", row=1, col=1)\\n\",\n",
    "    \"fig.update_yaxes(title_text=\\\"Confidence\\\", row=1, col=2)\\n\",\n",
    "    \"\\n\",\n",
    "    \"fig.update_layout(height=400, showlegend=False, title_text=\\\"Prediction Analysis\\\")\\n\",\n",
    "    \"fig.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## üß™ 8. Model Analysis & Insights\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Analyze model confidence on different sentiment strengths\\n\",\n",
    "    \"test_cases = {\\n\",\n",
    "    \"    'Strong Positive': [\\n\",\n",
    "    \"        \\\"Absolutely amazing! Best ever!\\\",\\n\",\n",
    "    \"        \\\"Incredible masterpiece!\\\",\\n\",\n",
    "    \"        \\\"Perfect in every way!\\\"\\n\",\n",
    "    \"    ],\\n\",\n",
    "    \"    'Mild Positive': [\\n\",\n",
    "    \"        \\\"Pretty good overall.\\\",\\n\",\n",
    "    \"        \\\"I liked it.\\\",\\n\",\n",
    "    \"        \\\"Worth watching.\\\"\\n\",\n",
    "    \"    ],\\n\",\n",
    "    \"    'Neutral': [\\n\",\n",
    "    \"        \\\"It was okay.\\\",\\n\",\n",
    "    \"        \\\"Not bad, not great.\\\",\\n\",\n",
    "    \"        \\\"Average experience.\\\"\\n\",\n",
    "    \"    ],\\n\",\n",
    "    \"    'Mild Negative': [\\n\",\n",
    "    \"        \\\"Could be better.\\\",\\n\",\n",
    "    \"        \\\"Not my favorite.\\\",\\n\",\n",
    "    \"        \\\"Somewhat disappointing.\\\"\\n\",\n",
    "    \"    ],\\n\",\n",
    "    \"    'Strong Negative': [\\n\",\n",
    "    \"        \\\"Absolutely terrible!\\\",\\n\",\n",
    "    \"        \\\"Worst ever!\\\",\\n\",\n",
    "    \"        \\\"Complete disaster!\\\"\\n\",\n",
    "    \"    ]\\n\",\n",
    "    \"}\\n\",\n",
    "    \"\\n\",\n",
    "    \"analysis_results = []\\n\",\n",
    "    \"\\n\",\n",
    "    \"for category, texts in test_cases.items():\\n\",\n",
    "    \"    for text in texts:\\n\",\n",
    "    \"        result = predict_sentiment(text, verbose=False)\\n\",\n",
    "    \"        analysis_results.append({\\n\",\n",
    "    \"            'Category': category,\\n\",\n",
    "    \"            'Text': text,\\n\",\n",
    "    \"            'Predicted': result['sentiment'],\\n\",\n",
    "    \"            'Confidence': result['confidence']\\n\",\n",
    "    \"        })\\n\",\n",
    "    \"\\n\",\n",
    "    \"analysis_df = pd.DataFrame(analysis_results)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Group by category\\n\",\n",
    "    \"category_stats = analysis_df.groupby('Category').agg({\\n\",\n",
    "    \"    'Confidence': ['mean', 'min', 'max']\\n\",\n",
    "    \"}).round(3)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\nüìä MODEL CONFIDENCE BY SENTIMENT STRENGTH:\\\")\\n\",\n",
    "    \"print(\\\"=\\\"*60)\\n\",\n",
    "    \"display(category_stats)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Visualize confidence by category\\n\",\n",
    "    \"plt.figure(figsize=(12, 6))\\n\",\n",
    "    \"\\n\",\n",
    "    \"categories = list(test_cases.keys())\\n\",\n",
    "    \"avg_confidences = [analysis_df[analysis_df['Category'] == cat]['Confidence'].mean() \\n\",\n",
    "    \"                   for cat in categories]\\n\",\n",
    "    \"\\n\",\n",
    "    \"colors_map = plt.cm.RdYlGn(np.linspace(0, 1, len(categories)))\\n\",\n",
    "    \"bars = plt.bar(categories, avg_confidences, color=colors_map, edgecolor='white', linewidth=2)\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.xlabel('Sentiment Strength', fontsize=12, weight='bold')\\n\",\n",
    "    \"plt.ylabel('Average Confidence', fontsize=12, weight='bold')\\n\",\n",
    "    \"plt.title('Model Confidence Across Different Sentiment Strengths', fontsize=14, weight='bold')\\n\",\n",
    "    \"plt.xticks(rotation=45, ha='right')\\n\",\n",
    "    \"plt.ylim(0, 1)\\n\",\n",
    "    \"plt.grid(axis='y', alpha=0.3)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Add value labels on bars\\n\",\n",
    "    \"for bar in bars:\\n\",\n",
    "    \"    height = bar.get_height()\\n\",\n",
    "    \"    plt.text(bar.get_x() + bar.get_width()/2., height,\\n\",\n",
    "    \"             f'{height:.2%}', ha='center', va='bottom', fontweight='bold')\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.tight_layout()\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## üíæ 9. Export Results\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Save batch results to CSV\\n\",\n",
    "    \"output_file = 'sentiment_analysis_results.csv'\\n\",\n",
    "    \"results_df.to_csv(output_file, index=False)\\n\",\n",
    "    \"print(f\\\"‚úÖ Results saved to: {output_file}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Save analysis results\\n\",\n",
    "    \"analysis_output = 'sentiment_strength_analysis.csv'\\n\",\n",
    "    \"analysis_df.to_csv(analysis_output, index=False)\\n\",\n",
    "    \"print(f\\\"‚úÖ Analysis saved to: {analysis_output}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## üéì 10. Try Your Own Text!\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Interactive prediction - modify the text below\\n\",\n",
    "    \"YOUR_TEXT = \\\"Enter your text here to analyze sentiment!\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"your_result = predict_sentiment(YOUR_TEXT)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## üìö 11. Model Information & Citation\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"print(\\\"\\\"\\\"\\n\",\n",
    "    \"‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\\n\",\n",
    "    \"‚ïë        Enhanced DistilBERT Sentiment Analysis Model          ‚ïë\\n\",\n",
    "    \"‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£\\n\",\n",
    "    \"‚ïë                                                               ‚ïë\\n\",\n",
    "    \"‚ïë  Model Architecture: DistilBERT + LoRA Adapters              ‚ïë\\n\",\n",
    "    \"‚ïë  Base Model: distilbert-base-uncased                         ‚ïë\\n\",\n",
    "    \"‚ïë  Enhancement: Knowledge Distillation from BERT-base          ‚ïë\\n\",\n",
    "    \"‚ïë  Training Dataset: SST-2 (67,349 samples)                    ‚ïë\\n\",\n",
    "    \"‚ïë  Validation Accuracy: 95.17"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
